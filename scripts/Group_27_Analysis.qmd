---
title: "Determinants of High Income: A Generalized Linear Model Analysis of 1994 US Census Socioeconomic Factors"
format: pdf
editor: visual
---

Group 27

## 1. Introduction

Income inequality remains a critical socioeconomic challenge. This study analyzes the 1994 US Census dataset using a generalized linear model (GLM) to identify key factors—such as age, education, occupation, and work hours—that significantly predict whether an individual earns over \$50k annually. The results aim to inform targeted policy interventions for improving economic equity.

```{r}
library(dplyr)
library(tidyverse)
library(broom)
library(ggplot2)
library(car) 
library(janitor)
library(broom) # Model results sorting
library(ResourceSelection) # Hosmer-Lemeshow test
library(pROC) # ROC curve
```

## 2. Data processing 

### 2.1 Data import 

Select the raw data, remove the NA value from the data, and determine the corresponding data type. Among all variables, only Age and Hours_PW are numerical variables. The integrated data is output to 'cleaned_dataset27' for preliminary judgment of data and variables. 

```{r}
# load data
data = read.csv("..\\datasets\\dataset27.csv")
```

```{r}
# inspect data
str(data)
summary(data)
sapply(data, unique)

# delete commas
# convert ? to NA
data <- data %>%
  mutate_all(~sub(",$", "", .)) %>%
  mutate_all(~na_if(., "?"))
```

```{r}
# inspect again
sapply(data, unique)
str(data)

# find that the type of Age and Hour_PW are char,
# then convert to num
data <- data %>%
  mutate_at(vars(Age, Hours_PW), as.numeric)
str(data)

# query and delete missing values
colSums(is.na(data))
data <- na.omit(data)
str(data)

# save data
write.csv(data, "..\\datasets\\cleaned_dataset27.csv", 
          row.names = FALSE)
```

### 2.2 Variable judgment 

By visualizing all the variables first, you can see that there are some variables with perfectly linear relationships among the classes (for example, Preschool in Education and Priv-house-serv in Occupation). 

```{r}
clean_data <- read.csv("..\\datasets\\cleaned_dataset27.csv")

# Relationship between categorical variables and income
lapply(c("Education", "Occupation", "Sex", "Marital_Status"), 
       function(var) {
         ggplot(clean_data, aes_string(x = var, fill = "Income")) +
           geom_bar(position = "fill") +
           coord_flip()
       })
```

### 2.3 New variable handling 

In order to avoid affecting the model fitting results later, we sorted and merged some data and consolidated it again into 'cleaned_dataset27_1.1.'. 

```{r}
# find the categories should be merged
table(clean_data$Nationality, clean_data$Income)


# merge categories in Education
clean_data <- clean_data %>%
  mutate(Education = case_when(
    Education %in% c("Preschool", "1st-4th", "5th-6th", "7th-8th", "9th") ~ "Basic Education",
    Education %in% c("10th", "11th", "12th", "HS-grad") ~ "High School",
    Education %in% c("Some-college", "Assoc-acdm", "Assoc-voc") ~ "College Education",
    Education == "Bachelors" ~ "Bachelors",
    Education %in% c("Masters", "Doctorate", "Prof-school") ~ "Advanced Education"
  ))

# merge categories in Marital_Status
clean_data <- clean_data %>%
  mutate(Marital_Status = case_when(
    Marital_Status %in% c("Married-AF-spouse", "Married-civ-spouse") ~ "Married",
    Marital_Status == "Never-married" ~ "Never-married", 
    Marital_Status %in% c("Divorced", "Married-spouse-absent", "Separated", "Widowed") ~ "Divorced/Spouse Absent/Separated/Widowed"
  ))

# merge categories in Occupation
clean_data <- clean_data %>%
  mutate(Occupation = case_when(
    Occupation %in% c("Exec-managerial", "Prof-specialty", "Tech-support") ~ "White-Collar", 
    Occupation %in% c("Craft-repair", "Machine-op-inspct", "Transport-moving") ~ "Blue-Collar", 
    Occupation %in% c("Handlers-cleaners", "Other-service", "Priv-house-serv", "Protective-serv") ~ "Service", 
    Occupation == "Adm-clerical" ~ "Adm-clerical",
    Occupation == "Sales" ~ "Sales", 
    Occupation == "Farming-fishing" ~ "Farming-fishing"
  ))

# merge categories in Nationality
clean_data <- clean_data %>%
  mutate(Nationality = case_when(
    Nationality == "United-States" ~ "United-States", 
    TRUE ~ "Others"
  ))

# save data
write.csv(clean_data, "..\\datasets\\cleaned_dataset27_1.1.csv", 
          row.names = FALSE)
```

## 3. Exploratory Data Analysis

### 3.1. Import data and packages

```{r}
# Import dataset
clean_data <- read.csv("..\\datasets\\cleaned_dataset27_1.1.csv")
```

### 3.2. Check data structure and summary statistics

```{r}
#| message: false
# Check structure
str(clean_data)

# Summary statistics
summary(clean_data)
```

The dataset contains 1376 observations with 8 variables. The data structure confirms that "Age" and "Hours_PW" are numerical, while other variables are categorical. The summary statistics show that the median age is 38, and the average work hours per week is around 41. Some extreme values are present, such as a maximum age of 90 and a maximum work hour of 99, which may require further investigation.

### 3.3. Visualize the income distribution

```{r}
#| message: false
# Bar plot for income distribution
ggplot(clean_data, aes(x = Income)) + 
  geom_bar(fill = "skyblue") +
  labs(title = "Income Distribution", x = "Income Category", y = "Count")
```

The chart shows that the majority of individuals have an income of \$50K or less. This indicates a class imbalance in the target variable, which could influence the performance of classification models.

### 3.4. Explore categorical variables' relationship with income

```{r}
#| message: false
# Stacked bar plots for categorical variables
categorical_vars <- c("Education", "Occupation", "Sex", "Marital_Status")

lapply(categorical_vars, function(var) {
  ggplot(clean_data, aes_string(x = var, fill = "Income")) +
    geom_bar(position = "fill") +
    coord_flip() +
    labs(title = paste("Income vs", var))
})
```

-   **Income vs. Education**

Higher education levels are associated with a higher proportion of individuals earning more than 50K. Advanced education and bachelor's degree holders have a significantly larger share of high-income earners compared to those with only basic or high school education.

-   **Income vs. Occupation**

White-collar and sales jobs have a higher percentage of individuals earning \>50K compared to blue-collar, service, and farming occupations. This suggests that occupation type plays a crucial role in income level.

-   **Income vs. Sex**

A higher percentage of males earn more than 50K compared to females. The income gap between genders suggests possible structural or occupational differences affecting earnings.

-   **Income vs. Marital Status**

Married individuals have a significantly higher proportion of high-income earners compared to those who are never married or divorced/separated/widowed. This could indicate that marital stability is associated with higher earnings, possibly due to dual-income households or other economic advantages.

### 3.5. Check category balance

```{r}
#| message: false
# Check proportion of income categories
prop.table(table(clean_data$Income))
```

The dataset is imbalanced, with about 75% of individuals earning ≤50K and 25% earning \>50K. This imbalance may affect model performance and should be considered when building predictive models.

### 3.6. Visualize numeric variables against income

```{r}
#| message: false
# Box plot for Hours per Week
ggplot(clean_data, aes(x = as.factor(Income), y = Hours_PW)) +
  geom_boxplot() +
  labs(title = "Income vs Hours Worked per Week", x = "Income", y = "Hours per Week")
```

Individuals with higher income (\>50K) tend to work more hours per week on average. The median work hours are higher for this group, and there are fewer individuals working very few hours compared to the lower-income group. However, there are some outliers working extreme hours in both income groups.

```{r}
#| message: false
# Box plot for Age
ggplot(clean_data, aes(x = as.factor(Income), y = Age)) +
  geom_boxplot() +
  labs(title = "Income vs Age", x = "Income", y = "Age")
```

Higher-income individuals tend to be older on average. The median age of the \>50K income group is higher than that of the ≤50K group. The distribution also shows a wider age range among high earners, though both groups have some outliers at older ages.

## 4. Model fitting

```{r}
data <- read.csv("..\\datasets\\cleaned_dataset27_1.1.csv")
```

### **4.1. Data preprocessing and variable conversion**

```{r}
#| label: data-prep
#| warning: false
#| message: false

# Convert categorical variables to factors
data <- data %>%
  mutate(
    Education = factor(Education),
    Marital_Status = factor(Marital_Status),
    Occupation = factor(Occupation),
    Sex = factor(Sex),
    Nationality = factor(Nationality),
    Income = factor(Income, levels = c("<=50K", ">50K"))
  )
```

We convert categorical variables (such as education, occupation) into factor types to ensure that the model correctly identifies categorical variables. The order of factors for the response variable Income is set to \<=50k for the base group, which affects the direction of interpretation of subsequent OR values.

### **4.2. Stepwise regression variable selection**

```{r}
#| label: model-selection
#| fig-height: 5

# Full variable logistic regression model
full_model <- glm(Income ~ Age + Education + Marital_Status + Occupation + 
                  Sex + Hours_PW + Nationality,
                family = binomial(link = "logit"), data = data)

# Stepwise regression selection variables (based on AIC)
step_model <- step(full_model, direction = "both")

# View the final model summary
summary(step_model)
```

**Variable filter logic:**

Step regression removes redundant variables by comparing AIC values (the smaller the better) : Nationality is first removed (AIC from 998.68→996.8), Sex is then removed (AIC from 996.8→996.1), and other variables (such as age and working hours) are retained in the model.

**Overall performance of the model:**

The residual deviation is reduced by 588.2 (1556.3→968.1), indicating that the model explains about 37.8% of the data variation AIC=996.1 is significantly lower than the original model's AIC=998.68, indicating that the optimization is successful

Interpretation of significant influencing factors

1.  **Age**

    Age 0.025275 p=0.000689 \*\*\*

    Practical significance: For each year of age increase, the chance of earning \>50k increases by 2.5% (OR=1.025).

2.  **Education level (Baseline: Advanced Education)**

    EducationBasic Education -2.720209 p=1.15e-05 \*\*\*

    EducationHigh School -1.077184 p=0.000653 \*\*\*

    Those with a basic education degree are 93% less likely to have a high income than those with a Advanced Education (OR=0.07) Those with a high school degree are 66% less likely to earn a high income than those with a PhD (OR=0.34)

3.  **Marital_Status (Base group: Single or Not married)**

    Married 2.626701 p\<2e-16 \*\*\*

    Married people are 13.8 times more likely to have a high income than the baseline group (OR=exp(2.626)=13.8)

4.  **Occupation type (Base group: Management)**

    Farm-fishing: Coefficient-2.845193 p = 0.000904 \*\*\* . It‘s less likely to earn a high income.

    White-Collar: Coefficient-2.845193 P = 0.000199 \*\*\*. It‘s more likely to earn a high income.

    Blue-Collar: Coefficient-0.155595, P = 0.622942. This does not significantly indicate a clear income difference from the baseline group after adjusting for other factors.

    The probability of high income of white-collar workers is 3.11 times that of the benchmark group (OR=exp(1.134)=3.11) Agriculture/fisheries workers were 94% less likely to have a high income than the baseline group (OR=0.06)

5.  **Working hours per week**

    Hours_PW 0.042542 p=3.06e-08 \*\*\*

    Working 10 more hours per week increases the likelihood of high income by 52% (OR=exp(0.042510)=1.52)

### **4.3. Model diagnosis and verification**

```{r}
#| label: model-diagnosis
#| fig-height: 6
#| out-width: "90%"

# Multicollinearity Detection (VIF)
vif(step_model) 

# Hosmer-Lemeshow goodness of fit test
hoslem.test(data$Income, fitted(step_model), g=5)
mean((predict(step_model, type="response") - (data$Income==">50K"))^2)# The smaller the better (0-0.25 is acceptable)


# Confusion matrix and accuracy
prob <- predict(step_model, type = "response")
pred <- ifelse(prob > 0.5, ">50K", "<=50K")
conf_matrix <- table(Predicted = pred, Actual=data$Income)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
accuracy

# ROC curve and AUC value
roc_curve <- roc(data$Income, prob)
plot(roc_curve, main = "ROC Curve")
auc(roc_curve)
```

1.  Multicollinearity diagnosis (GVIF results) The adjusted GVIF values (i.e., GVIF\^(1/(2\*Df)) are all \< 1.1 (well below the conservative threshold of 2), indicating that there is no significant multicollinearity between all variables (e.g., the association between education level and occupation is reasonably controlled). The results of model coefficient estimation are stable and reliable without deleting variables.

2.  We assessed model calibration using the Hosmer-Lemeshow test and Brier Score. After correcting for data format, the Hosmer-Lemeshow test indicated p-value is 0.8255814 and the Brier Score was 0.115, suggesting that the predicted probabilities were reasonably well-calibrated with the observed outcomes.

3.  Model differentiation ability (AUC=0.893) The model correctly identified 89.3% of the "high income versus low income" pairs.

4.  The accuracy value of model prediction is 0.8255814, indicating that about 82.56% of the prediction is correct. However, considering the imbalance of the sample sizes between two different incomes, ROC and AUC could be conducted as further detections. The ROC curve shows the curve bending sharply toward the top-left corner, which indicates high sensitivity and specificity. The AUC is 0.89, indicating the model is effective at distinguishing between income groups.

    ```{r}
    # Extract coefficient and OR value
    tidy_model <- tidy(step_model, conf.int = TRUE, exponentiate = TRUE)

    # Draw OR value forest map
    ggplot(tidy_model[-1, ], aes(x = estimate, y = term)) +
      geom_point() +
      geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
      geom_vline(xintercept = 1, linetype = "dashed") +
      labs(title = "Odds Ratio (OR) with 95% CI", x = "OR", y = "Variable")

    # Key variable effect chart
    data %>%
      ggplot(aes(x = Age, y = as.numeric(Income) - 1)) +
      geom_smooth(method = "glm", method.args = list(family = "binomial")) +
      labs(y = "Probability of Income >50K")

    data %>%
      ggplot(aes(x = Hours_PW, y = as.numeric(Income) - 1)) +
      geom_smooth(method = "glm", method.args = list(family = "binomial")) +
      labs(y = "Probability of Income >50K")

    ```

Trend judgment: Both Age and Hours_PW show a positive effect: the curve rises monotonically.

## 5. Conclusion 

### 5.1. Overall Model Performance

**Model Fit:** The final GLM achieved an AIC of 996.1 and residual deviance of 968.1, indicating strong explanatory power. 

**Predictive Accuracy:** The AUC of 0.893 demonstrates excellent discrimination between income groups. 

**Key Insight:** The model explains 37.8% of the variance in income levels (deviance reduction from 1,556.3 to 968.1). 

### 5.2. Non-Significant Factors

**Sex:** Gender showed no significant effect (p=0.348), suggesting income disparities in this dataset are better explained by Occupation/Education. 

**Nationality:** Birth nationality was excluded during model selection (AIC-optimized), indicating its limited predictive power. 

### 5.3. Limitations & Future Directions

**Temporal Bias:** Data reflects 1994 socioeconomic conditions; reanalysis with recent data is critical. 

**Unobserved Variables:** Regional cost-of-living differences and industry growth trends were not captured. 

**Nonlinear Effects:** Age and work-hour thresholds (e.g., diminishing returns beyond 50h/week) warrant further study. 

**Variables:** There are some variables in which the classification has perfect collinearity, and it may be possible to obtain a larger area of data to eliminate bias.
